# activations_fns_neural_nets

For commonly used activation functions include the following:
1. intialization:
2. regions of saturation:
3. avoid saturation:
4. activation progression with epochs:
5. gradient progression with epochs:
6. useful in:


Activations functions to include:
1. sigmoid
2. tanh
3. relu
4. leaky relu
5. softsign
6. max-out units
